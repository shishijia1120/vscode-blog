## 2 离散随机变量
### 2.1 基本概念

#### 与离散随机变量相关的概念

### 2.2 分布列
- 离散随机变量的取值概率是随机变量的最重要的特征.我们用分布列表示这种特征，并且用$p_x$表示随机变量$X$的分布列.
。。。
- 我们用大写字母表示随机变量，用小写字母表示实数.
对于分布列，我们有
$$\sum_x p_X{x}=1,$$

#### 随机变量$X$的分布列的计算
对每一个随机变量$X$的值$x$:
- 1. 找出与事件${X=x}$相对应的所有试验结果；
- 2. 将相应的试验结果的概率相加得到$p_X(x)$.

#### 2.2.1 伯努利随机变量
- 考虑抛掷一枚硬币，设正面向上的概率为$p$，反面向上的概率为$1-p$.伯努利随机变量在试验结果为正面向上时取值为1，在试验结果为反面向上时取值为0。

它的分布列为
$$p_X(k)=\begin{cases}
    p, \qquad 若k=1,\\
    1-p,\qquad 若k=0.\\
\end{cases}$$

用于刻画具有两个试验结果的概率模型.例如：
- 在给定的时刻，一架电话机可处于待机状态或使用状态;
- 一个人的政治态度

#### 2.2.2 二项随机变量
- 将一枚硬币抛掷$n$次，每次抛掷，正面出现的概率为$p$，反面出现的概率为$1-p$，而且各次抛掷是相互独立的.令$X$为$n$次抛掷得到正面的次数，我们称$X$为二项随机变量。其参数为$n$和$p$.$X$的分布列就是在1.5节讨论的二项概率:
$$p_X(k)=P(X=k)= \begin{array}
    n \\ k
\end{array}p^k (1-p)^{n-k},\quad k=0,1,\cdots,n.$$

利用归一化公理可得到
$$\sum_{k=0}^n  \begin{array}
    n \\ k
\end{array}p^k(1-p)^{n-k}=1.$$

二项分布图像。。。

#### 2.2.3 几何随机变量





#### 2.2.4 泊松随机变量
- 设随机变量$X$的分布列由下式给出
$$p_X(k)=e^{-\lambda}\frac{\lambda ^k}{k!},k=0,1,2,\cdots .$$

其中，$\lambda$是刻画分布列的取正值的参数，则称$X$是泊松随机变量.由于
$$\sum_{k=0}^\infty e^{-\lambda}\frac{\lambda^k}{k!}=e^{-\lambda}(1+\lambda+\frac{\lambda^2}{2!+\cdots})=e^{-\lambda}e^{\lambda}=1.$$

这个数列符合分布列的定义.

图
。。。




### 2.3 随机变量的函数
- 设$X$是一个随机变量.对$X$施行不同的变换，可以得到其他的随机变量.
- 设$Y=g(X)$是随机变量$X$的函数，由于对每一个试验结果，其对应的分布列为$p_X$，则$Y$也是离散随机变量，其分布列可通过$X$的分布列进行计算.实际上，对固定的$y$值，$p_Y(y)$的值可以通过下式计算
$$p_Y(y)=\sum_{\{x|g(x)=y\}}p_X(x).$$


例子：
。。。


### 2.4 期望、均值和方差
- $X$的分布列给出了$X$所以可能取值的概率.通常，我们希望将这些信息综合成一个能够代表这个随机变量的数.$X$的期望可以实现这个目的.$X$的期望就是$X$的所有取值相对于它的概率的加权平均.

#### 期望
- 设随机变量$X$的分布列为$p_X$，$X$的期望值由下式给出：
$$E[X]=\sum_x xp_X(x).$$

#### 2.4.1 方差、矩和随机变量的函数的期望规则
- 期望是随机变量及其分布列的重要特征.此外，还有其他重要的特征量.例如，随机变量$X$的二阶矩定义为随机变量$X^2$的均值.进一步$n$阶矩$E[X^n]$定义为$X^n$的期望值.这样均值本身就刚好是一阶矩.
- 除了均值，随机变量$X$的最重要的特征量是方差，记作$var(X)$.它由下式定义
$$var(X)=E[(X-E[X])^2].$$

方差提供了$X$在期望周围分散程度的一个测度.分散程度的另一个测度是标准差，它由下式定义
$$。。=\sqrt{var(X)}.$$

**计算方差**
- 1. 一种方法先行计算随机变量$(X-E[X])^2$的分布列，然后利用期望值的定义计算$X$的方差.$(X-E[X])^2$是随机变量$X$的函数，可利用前面提供的方法计算$(X-E[X])^2$的分布列.
- 2. 第二种方法计算$var(X)$时并不需要先行计算$(X-E[X])^2$的分布列，这种方法根据下面的规则得到
**随机变量的函数的期望规则**
- 设随机变量$X$的分布列为$p_X$，又设$g(X)$是$X$的一个函数，则$g(X)$的期望由下列公式得到
$$E[g(X)]=\sum_x g(x)p_X(x).$$

为验证此公式，令$Y=g(X)$并利用2.3节导出的公式
$$p_Y(y)=\sum_{\{x|g(x)=y\}}p_X(x).$$

得到
$$E[g(X)]=E[Y]$$

$$=\sum_y yp_Y(y)$$

$$=\sum_y y \sum_{\{x|g(x)=y\}} p_X(x)$$

$$=\sum_y  \sum_{\{x|g(x)=y\}} yp_X(x)$$

$$=\sum_y  \sum_{\{x|g(x)=y\}} g(x)p_X(x)$$

$$\sum_x g(x)p_X(x)$$

将期望规则应用到$X$的方差，我们得到
$$var(X)=E[(X-E[X])^2]=\sum_x (x-E[X])^2p_X(x).$$

相似地，对于$X$的$n$阶矩，我们有
$$E[X^n]=\sum_x x^np_X(x).$$

因此在计算$X$的$n$阶矩的时候，我们不必先求$X^n$的分布列.


- 先前已经提到，方差是非负的.那么是否可为0？由于在方差的公式$\sum_x (x-E[X])^2p_X(x)$中，每一项都是非负的.为了使得这个和式为0，其充要条件是对每一个$x$，$(x-E[X])^2p_X(x)=0.$这个条件说明对每一个使得$p_X(x)>0$的$x$，均有$x=E[X].$这说明$X$其实不是随机的，随机变量$X$等于$E[X]$的概率为1.

#### 2.4.2 均值和方差的性质

首先考虑随机变量$X$的函数
$$Y=aX+b,$$

其中$a$和$b$是已知常数.关于线性函数$Y$的均值和方差，我们有
$$E[Y]=\sum_x (ax+b) p_X(x)=a\sum_x xp_X(x)+b\sum_x p_X(x)=aE[X]+b.$$

进一步地
$$var(Y)=\sum_x (ax+b-E[aX+b])^2 p_X(x)$$

$$=\sum_x (ax+b-aE[X]-b)^2 p_X(x)$$

$$a^2\sum_x (x-E[X])^2 p_X(x)$$

$$=a^2var(X)$$


**随机变量的线性函数的均值和方差**
- 设$X$为随机变量，令
$$Y=aX+b,$$

其中$a$和$b$为给定的常数，则
$$E[Y]=aE[X]+b, var(Y)=a^2 var(X).$$

此外，我们还将证明下一个方差的重要公式.

**用矩表达的方差公式**
$$var(X)=E[X^2]-(E[X])^2.$$

这个用矩表达的方差公式的证明可以通过下列等式完成:
$$var(X)=\sum_x (x-E[X])^2 p_X(x)$$

$$=\sum_x (x^2-2xE[x]+(E[X])^2) p_X(x)$$

$$=\sum_x x^2p_X(x) -2E[X]\sum_x xp_X(x)+(E[X])^2\sum_x p_X(x)$$

$$E[X^2]-2(E[X])^2+(E[X])^2$$

$$E[X^2]-(E[X])^2.$$

。。。
#### 2.4.3 某些常用的随机变量的均值和方差
1. 伯努利随机变量的均值和方差



2. 离散均有随机变量



3. 泊松随机变量的均值









### 2.5 多个随机变量的联合分布列
- 设在同一个试验中有两个随机变量$X$和$Y$，它们的取值概率可以用它们的联合分布列刻画，并且用$p_{X,Y}$表示.
$$p_{X,Y}=P(X=x,Y=y).$$

- 利用联合分布列可以确定任何由随机变量$X$和$Y$所刻画的事件的概率.例如$A$是某些$(x,y)$所形成的集合，则
$$P((X,Y)\in A)=\sum_{(x,y)\in A}P_{X,Y}(x,y).$$

事实上，我们还可以利用$X$和$Y$的联合分布列计算$X$或$Y$的分布列
$$P_X(x)=\sum_y P_{X,Y}(x,y), \qquad P_Y(y)=\sum_x P_{X,Y}(x,y).$$

我们称$P_X(x)$或$P_Y(y)$为边缘分布列.

#### 2.5.1 多个随机变量的函数
- 从二元函数$Z=g(X,Y)$可以确定一个新的随机变量，这个新的随机变量的分布列可以从联合分布列通过下式计算
$$p_Z(z)=\sum_{\{(x,y|g(x,y)=z\}}p_{X,Y}(x,y).$$

进一步地，关于随机变量的函数的期望规则可以推广成下列形式
$$E[g(X,Y)]=\sum_x \sum_y g(x,y)p_{X,Y}(x,y).$$

这个公式的证明与单变量函数的公式的证明类似.特别地，当$g$是形如$aX+bY+c$的线性函数的时候，我们有
$$E[aX+bY+c]=aE[X]+bE[Y]+c,$$

其中$a,b,c$均为给定的常数.

#### 2.5.2 多于两个随机变量的情况
- 设有三个随机变量$X,Y,Z$，其联合分布列的定义是类似的，即
$$p_{X,Y,Z}=P(X=x,Y=y,Z=z),$$

相应地可以得到边缘分布列，例如
$$p_{X,Y}=\sum_x p_{X,Y,Z}(x,y,z),$$

$$p_{X}=\sum_y \sum_x p_{X,Y,Z}(x,y,z),$$


关于随机变量的函数的期望规则为：
$$E[g(X,Y,Z)]=\sum_x \sum_y \sum_z g(x,y,z)p_{X,Y,Z}(x,y,z),$$

并且，如果$g$是形如$aX+bY+cZ+d$的线性函数，则
$$E[aX+bY+cZ+d]=aE[X]+bE[Y]+cE[Z]+d.$$

**关于联合分布列的小结**
设$X$和$Y$为在某个试验中的随机变量.
- $X$和$Y$的联合分布列$p_{X,Y}$由下式定义
$$p_{X,Y}(x,y)=P(X=x,Y=y).$$

- $X$和$Y$的边缘分布列可由下式得到
$$p_X(x)=\sum_y p_{X,Y}(x,y), \quad p_Y(y)=\sum_x p_{X,Y}(x,y).$$

- $X$和$Y$的函数$g(X,Y)$是一个随机变量，并且
$$E[g(X,Y)]=\sum_x \sum_y g(x,y)p_{X,Y}(x,y),$$

若$g$是线性的，且$g=aX+bY+c,$则
$$E[aX+bY+c]=aE[X]+bE[Y]+c.$$

- 上面的结论可以自然地推广到两个以上的随机变量的情况.

### 2.6 条件
#### 2.6.1 某个事件发生的条件下的随机变量
- 在某个事件$A(P(A)>0)$发生的条件下，随机变量$X$的条件分布列由下式定义：
$$p_{X|A}(x)=P(X=x|A)=\frac{P({X=x} \cap A)}{P(A)}$$

注意，对于不同的$x,{X=x} \cap A$是互不相容的事件，它们的并为$A$.因此
$$P(A)=\sum_x P({X=x} \cap A).$$

比较得到的两个式子，可以看出
$$\sum_x p_{X|A}(x)=1.$$

故$p_{X|A}$符合分布列的要求.

#### 2.6.2 给定另一个随机变量的值的条件下的随机变量
- 设某一个试验中有两个随机变量$X$和$Y$.我们假定随机变量$Y$已经取定一个值$y(p_Y(y)>0)$，这个$y$值提供了关于$X$取值的部分信息.这些信息包含于$X$的给定$Y$的值的条件分布列$p_{X|Y}$中.所谓条件分布列就是$p_{X|A}$，其中事件$A$就是事件$\{Y=y\}:$
$$p_{X|Y}(x|y)=P(X=x|Y=y).$$

利用条件概率的定义，我们有
$$p_{X|Y}(x|y)=\frac{P(X=x,Y=y)}{P(Y=y)}=\frac{p_{X,Y}(x,y)}{p_Y (y)}.$$

$p_{X|Y}(x|y)$满足条件
$$\sum_x p_{X|Y}(x|y)=1.$$

利用公式
$$P_{X,Y}(x,y)=P_Y(y)p_{X|Y}(x|y),$$

或利用
$$P_{X,Y}(x,y)=P_X(x)p_{Y|X}(y|x).$$

**关于条件分布列的小结**
。。。


#### 2.6.3 条件期望
- 条件分布列就是一个通常的分布列，不过它的样本空间由条件期望所限定的试验结果组成，相应的事件的概率变成条件概率.同样的原因，条件期望就是通常的期望.

**关于条件期望的小结**
设$X$和$Y$为某一试验中的两个随机变量.
- 设$A$为某事件，$P(A)>0.$随机变量$X$在给定$A$发生的条件下的条件期望为
$$E[X|A]=\sum_x x p_{X|A}(x).$$
对于函数$g(X)$,我们有
$$E[g(X)|A]=\sum_x g(x) p_{X|A}(x).$$

- 给定$Y=y$的条件下$X$的条件期望由下式定义
$$E[X|Y=y]=\sum_x x p_{X|Y}(x|y).$$
- 设$A_1,\cdots,A_n$是互不相容的事件并形成样本空间的一个分割，假定$P(A_i)>0$对一切$i$成立，则
$$E[X]=\sum_{i=1}^n P(A_i)E[X|A_i].$$

进一步假定事件$B$满足对一切$i。P(A_i \cap B)>0$，则
$$E[X|B]=\sum_{i=1}^n P(A_i|B)E[X|A_i \cap B].$$

- 我们有
$$E[X]=\sum_y p_Y(y)E[X|Y=y].$$

上述最后的三个等式适用于不同的场合，但它们本质上是相互等价的.它们都可以称为全期望定理.这些定理表达了这样的一个事实："无条件平均可以由条件平均再求平均得到."通过全期望定理可利用条件分布列或条件期望计算无条件期望$E[X]$.现在验证三个公式中的第一个公式.先写出全概率公式
$$p_{x}(x)=\sum_{i=1}^n P(A_i) p_{x|A_i} (x|A_i),$$

再在两边乘$x$并对一切$x$求和，得到
$$E[X]=\sum_x xp_X(x)$$

$$=\sum_x x \sum_{i=1}^n P(A_i)p_{x|A_i}(x|A_i)$$

$$=\sum_{i=1}^n P(A_i) \sum_x x  p_{x|A_i}(x|A_i)$$

$$\sum_{i=1}^n P(A_i) E[X|A_i].$$

### 2.7 独立性
#### 2.7.1 随机变量与事件的相互独立性
- 随机变量与事件的独立性的概念与两个事件的相互独立性的概念是相同的.其基本思想是刻画条件的事件的发生与否不会对随机变量取值提供新的信息.更具体地说，随机变量$X$独立于事件$A$是指
$$P(X=x 且A)=P(X=x)P(A)=p_X(x)P(A)对一切x成立,$$

这个条件等价于，对任何$x$，随机事件${X=x}$与事件$A$相互独立.由条件分布列的定义.
$$P(X=x 且 A)=P_{X|A}(x)P(A).$$

所以，只要$P(A)>0$，随机变量$X$与事件$A$的独立性与下面的条件是等价的：
$$p_{X|A}(x)=p_X(x)对一切x成立.$$

#### 2.7.2 随机变量之间的相互独立性
- 随机变量之间相互独立性与随机变量和随机事件的相互独立性的概念是完全相同的.随机变量$X$和$Y$称为相互独立的随机变量.若它们满足
$$p_{X,Y}(x,y)=p_X(x)P_Y(y)对一切x和y成立.$$

由公式$p_{X,Y}(x,y)=p_{X|Y}(x|y)p_Y(y)$可知随机变量$X$和$Y$的相互独立性的条件等价于
$$p_{X|Y}(x|y)=p_X(x) 对一切x和一切满足p_Y(y)>0的y成立$$

直观上，$Y$和$X$的独立性意味着$Y$的取值不会提供$X$取值的信息.

- 在给定事件$A$的条件下$(P(A)$必须大于0!)
。。。

#### 2.7.3 几个随机变量的相互独立性
- 我们称随机变量$X、Y$和$Z$是三个相互独立得随机变量，如果它们满足
$$p_{X,Y,Z}(x,y,z)=p_X(x)p_Y(y)p_Z(z)对一切x,y,z成立.$$

。。。

#### 2.7.4 若干个相互独立的随机变量的和的方差
- 设$X_1,\cdots,X_n$为相互独立的随机变量序列，则
$$var(X_1+\cdots+X_n)=var(X_1)+\cdots+var(X_n).$$

这个结论可以通过反复应用两个随机变量之和的方差公式$var(X+Y)=var(X)+var(Y)$而证得.






