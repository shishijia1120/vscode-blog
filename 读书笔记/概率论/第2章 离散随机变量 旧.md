## 2 离散随机变量
### 2.1 基本概念
----
**与随机变量相关的主要概念**

$在一个试验的概率模型之下$
- $随机变量是试验结果的实值函数；$
- $随机变量的函数定义了另一个随机变量；$
- $对于一个随机变量，我们可以定义一些平均量，例如均值和方差；$
- $可以在某事件或某随机变量的条件之下定义一个随机变量；$
- $存在一个随机变量与某事件或某随机变量相互独立的概念；$

----
**与离散随机变量相关的概念**

P85

----

### 2.2 分布列
----
$$离散随机变量的取值概率是随机变量的最重要特征。我们用分布列表示这种特征，并且用PX表示随机变量X的分布列。$$

$$设x是随机变量X的取值，则X取值为x的概率定义为事件{X=x}的概率，即所有与x对应的试验结果所组成的事件的概率，$$

$用P_X(x)表示，即$

$$P_X(x)=P({X=x}).$$

$我们用大写字母表示随机变量，用小写字母表示实数。$

$对于分布列，我们有$
$$\sum_x P_X(x)=1.$$
$上式之所以成立是由于概率的可加性和归一性公理。$


----
#### 2.2.1 伯努利随机变量

#### 2.2.2 二项随机变量

#### 2.2.3 几何随机变量
#### 2.2.4 泊松随机变量



### 2.3 随机变量的函数
----
$$设Y=g(X)是随机变量X的函数，由于对每一个试验结果，也对应一个(Y的)数值，故Y本身也是一个随机变量。$$

$$如果X是离散的随机变量，其对应的分布列为P_X，则Y也是离散随机变量，其分布列可通过X的分布列进行计算。$$

$实际上，对固定的y值，P_Y(y)的值可以通过下式计算$

$$P_Y(y)=\sum_{\{x|g(x)=y\}}P_X(x).$$


### 2.4 期望、均值和方差
----
**期望**
$设随机变量X的分布列为P_X，X的期望值由下式给出：$
$$E[X]=\sum_x xP_X(x).$$

#### 2.4.1 方差、矩和随机变量的函数的期望规则
----

$$期望是随机变量及其分布列的重要特征，此外，还有其它重要的特征量。$

$$例如随机变量X的二阶矩定义为随机变量X^2的均值。进一步n阶矩E[X^n]定义为X^n的期望值。$$

$这样的均值本身就刚好是一阶矩。$
**方差**
$$var(X)=E[(X-E[X])^2].$$
$方差提供了X在期望值周围分散程度的一个测度，分散程度的另一个测度是标准差，它由下式定义$
$$ \sigma _X=\sqrt{var(X)}.$$

$标准差具有实用性，因为它的量纲与X的相同。例如X是以米为单位的长度，方差的单位为平方米，$
$而标准差的单位为米。$

$$计算方差的一种方法是先行计算随机变量(X-E[X])^2的分布列，然后利用期望值的定义计算X的方差。$$

$$(X-E[X])^2是随机变量X的函数，可利用前面提供的方法计算(X-E[X])^2的分布列。$$

----
**随机变量的函数的期望规则**
$$设随机变量X的分布列为P_X，又设g(X)是X的一个函数，则g(X)的期望由下列公式得到$$

$$E[g(X)]=\sum_x g(x)P_X(x).$$

----
$为验证此公式，令Y=g(X)并利用2.3节导出的公式$
$$P_Y(y)=\sum_{\{x|g(x)=y\}}P_X(x),$$

$得到$
$$E[g(X)]=E[Y]$$

$$=\sum_yyP_Y(y)$$

$$=\sum_yy\sum_{\{x|g(x)=y\}}P_X(x)$$

$$=\sum_y\sum_{\{x|g(x)=y\}}yP_X(x)$$

$$=\sum_y\sum_{\{x|g(x)=y\}}g(x)P_X(x)$$

$$=\sum_x g(x)P_X(x).$$
$将期望规则应用到X的方差，我们得到$
$$var(X)=E[(X-E[X])^2]=\sum_x(x-E[X])^2P_X(x).$$

$相似地，对于X的n阶矩，我们有$
$$E[X^n]=\sum_x x^nP_X(x).$$

$因此在计算X的n阶矩的时候，我们不必先求X^n的分布列。$

----
**方差**
$随机变量X的方差由下列公式所定义:$
$$var(X)=E[(X-E[X])^2].$$
$并且可以用下式进行计算：$
$$var(X)=\sum_x(x-E[X])^2P_X(x).$$
$它是非负的，其平方根称为标准差，记为\sigma_X.$

#### 2.4.2 均值和方差的性质
----
$$我们将用随机变量的函数的期望规则导出一些均值和方差的重要性质。首先考虑随机变量X的函数$$

$$Y=aX+b,$$

$其中a和b是已知常数，关于线性函数Y的均值和方差，我们有$
$$E[Y]=\sum_x(ax+b)P_X(x)=a\sum_x xP_X(x)+b\sum_xP_X(x)=aE[X]+b.$$

$进一步地$
$$var(Y)=\sum_x(ax+b-E[aX+b])^2P_X(x)$$

$$=\sum_x(ax+b-aE[aX+b])^2P_X(x)$$

$$=a^2\sum_x(x-E[X])^2P_X(x)$$

$$=a^2var(X).$$

----
**随机变量的线性函数的均值和方差**
$设X为随机变量，令$
$$Y=aX+b,$$

$其中a和b为给定的常数，则$
$$E[Y]=aE[X]+b,var[Y]=a^2var(X).$$

----

**用矩表达的方差公式**
$$var(X)=E[X^2]-(E[X])^2.$$
$这个用矩表达的方差公式的证明可以通过下列等式完成：$
$$var(X)=\sum_x(x-E[X])^2P_X(x)$$

$$=\sum_x(x^2-2xE[X]+(E[X])^2)P_X x$$

$$=\sum_x x^2P_X(x)-2E[X]\sum_xxP_X(x)+(E[X])^2\sum_xP_X(x)$$

$$=E[X^2]-2(E[X])^2+(E[X])^2$$

$$=E[X^2]-(E[X])^2.$$


> **注意**：$除非g(X)是一个线性函数，一般情况下E[g(X)]不等于g(E[X]).$



#### 2.4.3 某些常用的随机变量的均值和方差
----
$1. 伯努利随机变量的均值和方差$
$$P_X(k)=\begin{cases}
    p, \qquad  \quad若k=1，\\
    1-p, \quad若k=0.
\end{cases}$$


$E[X]=1 \cdot p+0\cdot(1-p)=p,$
$E[X^2]=1^2 \cdot p+0^2\cdot(1-p)=p,$
$var(X)=E[X^2]-(E[X])^2=p-p^2=p(1-p).$

----

$2. 离散均匀随机变量$
$$P_X(k)=\begin{cases}
    \frac{1}{b-a+1}, \quad 若k=a,a+1,\cdots,b, \\
    0,\qquad其他.
\end{cases}$$
$其中a,b是两个整数，作为随机变量的值域的两个端点，a<b。由于它的分布列相对于(a+b)/2是对称的，其均值为$
$$E[X]=\frac{a+b}{2}.$$

$为计算X的方差，先考虑a=1和b=n的简单情况。利用归纳法可以证明$
$$E[X^2]=\frac{1}{n}\sum_{k=1}^n k^2=\frac{1}{6}(n+1)(2n+1)$$

$这样利用一、二阶矩，可得到X的方差$

$$var(X)=E[X^2]-(E[X])^2$$

$$\qquad \qquad \qquad \qquad \quad=\frac{1}{6}(n+1)(2n+1)-\frac{1}{4}(n+1)^2$$

$$=\frac{n^2-1}{12}$$

$E[X]=\frac{a+b}{2}, \qquad var(X)=\frac{(b-a)(b-a+2)}{12}$

$var(X)=\frac{(b-a+1)^2-1}{12}=\frac{(b-a)(b-a+2)}{12}.$

----
$3. 泊松随机变量的均值$
$$P_X(k)=e^{-\lambda}\frac{\lambda^k}{k!},k=0,1,2,\cdots,$$
$其中\lambda>0为常数。其均值可从下列等式得到$
$$E[X]=\sum_{k=0}^\infty ke^{-\lambda}\frac{\lambda^k}{k!}$$

$$E[X]=\sum_{k=1}^\infty ke^{-\lambda}\frac{\lambda^k}{k!} \qquad (k=0这一项为0)$$

$$=\sum_{k=1}^{\infty}ke^{-\lambda}\frac{\lambda^{k-1}}{(k-1)!}$$

$$\qquad \qquad \qquad \quad=\lambda\sum_{m=0}^\infty e^{-\lambda}\frac{\lambda^m}{m!} \qquad (令m=k-1)$$

$$=\lambda.$$

$最后一个等式利用了泊松分布的归一化性质。$

#### 2.4.4 利用期望值进行决策
见p102页


### 2.5 多个随机变量的联合分布列


----

#### 2.5.1 多个随机变量的函数
$$存在多个随机变量的情况下，就要可能从这些随机变量出发构造出新的随机变量。$$

$$特别地，从二元函数Z=g(X,Y)可以确定一个新的随机变量。这个新的随机变量的分布列可以从联合分布列通过下式计算$$
$$P_Z(z)=\sum_{\{(x,y)|g(x,y)=z\}}P_{X,Y}(x,y).$$
$进一步地，关于随机变量的函数的期望规则可以推广成下列形式$
$$E[g(X,Y)]=\sum_x\sum_yg(x,y)P_{X,Y}(x,y).$$

$$这个公式的证明与单变量函数的公式的证明类似。特别地，当g是形如aX+bY+c的线性函数的时候，我们有$$

$$E[aX+bY+c]=aE[X]+bE[Y]+c,$$

$其中a,b,c均为给定的常数.$


----

#### 2.5.2 多于两个随机变量的情况
$设有三个随机变量X,Y,Z，其联合分布列的定义是类似的，即$
$$P_{X,Y,Z}(x,y,z)=P(X=x,Y=y,Z=z),$$

$$其中(x,y,z)是(X,Y,Z)的所有可能的取值，相应地可以得到边缘分布列，例如$$

$$P_{X,Y}=\sum_zP_{X,Y,Z}(x,y,z),$$

$$P_X(x)=\sum_y \sum_zP_{X,Y,X}(x,y,x).$$
$关于随机变量的函数的期望规则为$
$$E[g(X,Y,Z)]=\sum_x \sum_y \sum_zg(x,y,z)P_{X,Y,Z}(x,y,z),$$

$并且，如果g是形如aX+bY+cZ+d的线性函数，则E[aX+bY+cZ+d]=aE[X]+bE[Y]+cE[Z]+d.$
$$进一步地，上面的结果可以推广到三个以上随机变量的情况。例如设X_1,X_2,\cdots,X_n为n个随机变量，$$

$a_1,a_2,\cdots,a_n为n个常数，我们有$

$$E[a_1X_1+a_2X_2+\cdots+a_nX_n]=a_1E[X_1]+a_2E[X_2]+\cdots+a_nE[X_n].$$

----
**关于联合分布列的小结**

$设X和Y为在某个试验中的随机变量.$

- $X和Y的联合分布列P_{X,Y}由下式定义$
$$P_{X,Y}(x,y)=P(X=x,Y=y).$$

- $X和Y的边缘分布列可由下式得到$
$$P_X(x)=\sum_yP_{X,Y}(x,y), \qquad P_Y(y)=\sum_xP_{X,Y}(x,y).$$
- $X和Y的函数g(X,Y)是一个随机变量，并且$
$$E[g(X,Y)]=\sum_x\sum_yg(x,y)P_{X,Y}(x,y).$$

$\qquad 若g是线性的，且g=aX+bY+c，则$
$$E[aX+bY+c]=aE[X]+bE[Y]+c.$$
- $上面的结论可以自然地推广到两个以上的随机变量的情况.$

### 2.6 条件

----

#### 2.6.1 某个事件发生的条件下的随机变量
$$在某个事件A(P(A)>0)发生的条件下，随机变量X的条件分布列由下式定义：$$
$$P_{X|A}(x)=P(X=x|A)=\frac{P({X=x}\cap A)}{P(A)}.$$

$$注意，对于不同的x，{X=x}\cap A是互补相容的事件，它们的并为A。因此$$
$$P(A)=\sum_xP({X=x}\cap A).$$
$比较得到的两个式子，可以看出$
$$\sum_xP_{X|A}(x)=1,$$
$故P_{X|A}符合分布列的要求.$
$$条件分布列的计算也与无条件分布列的计算一样，将满足X=x并且属于A的试验结果的概率相加，$$

$最后除以P(A)，便得到P_{X|A}(x)的值。$


#### 2.6.2 给定另一个随机变量的值的条件下的随机变量

----
$$设某一个试验中有两个随机变量X和Y，我们假定随机变量Y已经取定一个值y(P_Y(y)>0)，$$

$$这个y值提供了关于X取值的部分信息。这些信息包含于X的给定Y的值的条件分布列P_{X|Y}中。$$

$所谓条件分布列就是P_{X|A}，其中事件A就是事件{Y=y}:$

$$P_{X|Y}(x|y)=P(X=x|Y=y).$$

$利用条件概率的定义，我们有$
$$P_{X|Y}(x|y)=\frac{P(X=x,Y=y)}{P(Y=y)}=\frac{P_{X,Y}(x,y)}{P_Y(y)}.$$

----
**关于条件分布列的小结**
。。。。。。

----

#### 2.6.3 条件期望

$$条件分布列就是一个通常的分布列，不过它的样本空间由条件所限定的试验结果组成，相应的事件的概率变成条件概率。$$

$$同样的原因，条件期望就是通常的期望，不过试验结果的空间由条件所限定的试验结果所组成。$$

$$相应的概率和分布列都换成条件概率和条件分布列(关于条件方差的处理是完全类似的)。$$

----
**关于条件期望的小结**

$设X和Y为某一试验中的两个随机变量。$

- $设A为某事件，P(A)>0.随机变量X在给定A发生的条件下的条件期望为$

$$E[X|A]=\sum_x xP_{X|A}(x).$$

$\qquad 对于函数g(X)，我们有$
$$E[g(X)|A]=\sum_xg(x)P_{X|A}(x).$$

- $给定Y=y的条件下X的条件期望由下式定义$
$$E[X|Y=y]=\sum_x xP_{X|Y}(x|y)。$$

- $$设A_1,\cdots,A_n是互不相容的事件并且形成样本空间的一个分割，假定P(A_i)>0对一切i成立。则$$

$$E[X]=\sum_{i=1}^nP(A_i)E[X|A_i].$$

$\qquad
 进一步假定事件B满足对一切i，P(A_i \cap B)>0，则$
$$E[X|B]=\sum_{i=1}^nP(A_i|B)E[X|A_i \cap B].$$

- $我们有$
$$E[X]=\sum_y P_Y(y)E[X|Y=y].$$


----
$$上述最后的三个等式适用于不同的场合，但它们本质上是相互等价的。它们都可以称为全期望定理。$$

$$这些定理表达了这样的一个事实："无条件平均可以由条件平均再求平均得到。"通过全期望定理可利用条件分布列$$

$$或条件期望计算无条件期望E[X]。现在验证三个公式中的第一个公式，先写出全概率公式：$$

$$P_X(x)=\sum_{i=1}^nP(A_i)P_{X|A_i}(x|A_i),$$

$再在两边乘x并对一切x求和，得到$
$$E[X]=\sum_xxP_X(x)$$

$$\qquad \qquad \qquad \qquad =\sum_x x\sum_{i=1}^nP(A_i)P_{X|A_i}(x|A_i)$$

$$\qquad \qquad \qquad \qquad=\sum_{i=1}^nP(A_i)\sum_x xP_{X|A_i}(x|A_i)$$

$$\qquad \qquad \quad =\sum_{i=1}^nP(A_i)E[X|A_i].$$

$其他两个公式的验证是类似的。$


### 2.7 独立性

----
#### 2.7.1 随机变量与事件的相互独立性
$$随机变量与事件的独立性的概念与两个事件的相互独立性的概念是相同的。其基本思想是刻画条件的事件的发生$$

$$与否不会对随机变量取值提供新的信息。更具体地说，随机变量X独立于事件A是指$$

$$P(X=x且A)=P(X=x)P(X=x)P(A)=P_XP(A)对一切x成立.$$

$$这个条件等价于：对任何X，随机事件{X=x}与事件A相互独立。由条件分布列的定义，$$

$$P(X=x且A)=P_{X|A}P(A)，$$

$$所以，只要P(A)>0，随机变量X与事件A的独立性与下面的条件是等价的：$$

$$P_{X|A}(x)=P_X(x)对一切x成立.$$

#### 2.7.2 随机变量之间的相互独立性
----



。。。。。。


----
**关于独立随机变量的性质的小结**
$$设在某一试验中，A是一个事件，满足条件P(A)>0，又设X和Y是在同一个试验中的两个随机变量。$$

- $$称X为相对于事件A独立，如果满足P_{X|A}(x)=P_X(x)对一切x成立，即对一切x，事件{X=x}与A相互独立。$$

- $$称X和Y为相互独立的随机变量，如果对一切可能的数对(x,y)，事件{X=x}和{Y=y}相互独立，$$

$$或等价地P_{X,Y}(x,y)=P_X(x)P_Y(y)对一切x和y成立。$$

- $$若X和Y相互独立，则E[XY]=E[X]E[Y].进一步地，对于任意函数g和h，随机变量g(X)和h(Y)也是相互独立的，$$

$并且E[g(X)h(Y)]=E[g(X)]E[h(Y)].$
- $若X和Y相互独立，则var(X+Y)=var(X)+var(Y).$

#### 2.7.3 几个随机变量的相互独立性
----

$$前面的关于随机变量相互独立的讨论可以很自然地推广到两个以上随机变量相互独立的情况。$$

$$例如，我们称随机变量X、Y、Z是三个相互独立的随机变量，如果它们满足$$

$$P_{X,Y,Z}(x,y,z)=P_X(x)P_Y(y)P_Z(z)对一切x,y,z成立。$$

$$设X、Y、Z是三个相互独立的随机变量，则任何形如f(X)、g(Y)、h(Z)的三个随机变量也是相互独立的。相似地，$$

$$任何两个随机变量g(X,Y)和h(Z)也是相互独立的。但是形如g(X,Y)和h(Y,Z)的两个随机变量通常不是相互独立的，$$

$$因为它们都受公共的随机变量Y的影响。若用互不干扰的试验结果来解释独立性，则上述这些性质在直观上是非常清楚的。$$

$$但是形式的证明有些繁琐。幸运的是，直观和数学理论通常是一致的。这主要是，独立性的定义本身反映了对直观的解释。$$


#### 2.7.4 若干个相互独立的随机变量的和的方差
----
$$相互独立的随机变量的和出现在许多重要的场合。例如在测量问题中，为了减少测量误差，通常是把若干个独立$$

$$的测量值的平均值作为目标物的测量值。在处理若干个相互独立的随机源的累计效果时，也会遇到随机变量和的方差问题。$$

$$在以下的例子中，我们将利用下面的重要性质：设X_1,\cdots,X_n为相互独立的随机变量序列，则$$

$$var(X_1+\cdots+X_n)=var(X_1)+\cdots+var(X_n).$$

$$这个结论可以通过反复应用两个独立随机变量之和的方差公式var(X+Y)=var(X)+var(Y)而证得。$$

### 2.8 小结和讨论
----
$$本章集中讨论离散随机变量，为离散随机变量建立了理论架构和引进了相应的工具。$$

$$特别地，我们引入了一些基本概念，例如分布列、均值和方差。这些概念在不同程度上刻画了离散随机变量的概率特征。$$

$$同时，我们指出，为了计算Y=g(X)的期望和方差，可以不用Y的分布列，而只需利用X的分布列即可。$$

$$特别地，当g是一个线性函数Y=aX+b的时候，X和Y的期望和方差具有下列关系$$

$$E[Y]=aE[X]+b, \qquad var(Y)=a^2var(X).$$

$$我们也讨论了若干具体的离散随机变量，并且导出了分布列、均值和方差，其结果如下。$$

----
**某些具体的离散随机变量的小结**
- $[a,b]上的离散均匀分布(a,b为整数)$
$$P_X(k)=\begin{cases}
    \frac{1}{b-a+1}, \qquad若k=a,a+1,\cdots,b, \\
    0, \qquad \qquad 其他，
\end{cases}$$

$$\qquad E[X]=\frac{a+b}{2}, \qquad var(X)=\frac{(b-a)(b-a+2)}{12}.$$

- $参数为p的伯努利随机变量(刻画一次试验成功或失败的概率模型)$

$$P_X(k)=\begin{cases}
    p, \qquad 若k=1, \\
    1-p,\quad 若k=0,
\end{cases}$$

$$E[X]=p, \qquad var(x)=p(1-p).$$

- $参数为p和n的二项随机变量(刻画n次独立重复的伯努利试验中成功次数的随机变量)$
$$P_X(k)=\begin{pmatrix}
    n \\ k
\end{pmatrix}p^k(1-p)^{n-k}， \quad k=0,1,\cdots,n,$$

$$E[X]=np, \qquad var(X)=np(1-p).$$

- $$参数为p的几何随机变量(在独立同分布的伯努利试验序列中刻画直到第一次成功所需的试验次数的随机变量)$$
  
$$P_X(k)=(1-p)^{k-1}p, \qquad k=1,2,\cdots,$$

$$E[X]=\frac{1}{p}, \qquad var(X)=\frac{1-p}{p^2}.$$

- $参数为\lambda的泊松随机变量(当n很大，p很小，\lambda=np时，用于逼近二项分布的随机变量)$
$$P_X(k)=e^{-\lambda}\frac{\lambda^k}{k!}, \qquad k=0,1,2,\cdots,$$

$$E[X]=\lambda, \qquad var(X)=\lambda.$$

----
$$我们也讨论了多元随机变量和它的联合分布列和条件分布列，以及与之相关的条件期望。$$

$$条件分布列通常还是定义一个概率模型的起始点，它可以用来计算其他的概率值，例如边缘分布列或联合分布列或相应的期望值。$$

$$特别地，当条件分布列P_{X|Y}(x|y)给定以后，有以下几种情形。$$
- (a) $X,Y的联合分布列可由下式计算$
    $$P_{X,Y}(x,y)=P_Y(y)P_{X|Y}(x|y).$$

$\qquad 这个结果可以推广到多于两个变量的情况，例如：$
$$P_{X,Y,Z}(x,y,z)=P_Z(z)P_{Y|Z}(y|z)P_{X|Y,Z}(x|y,z).$$

$\qquad这个公式与第1章中利用序贯树形图计算概率的方法类似。$

- (b) $X的边缘分布列可用下式计算：$
  $$P_X(x)=\sum_yP_Y(y)P_{X|Y}(x,y).$$

  $\qquad 这个公式与第1章中的全概率公式类似。$

- (c) $(b)中的全概率公式可以推广成为全期望公式，以计算随机变量X的期望：$
$$E[X]=\sum_yP_Y(y)E[X|Y=y].$$

$\qquad 类似于事件的相互独立性，我们也引进了独立随机变量的概念。特别地，我们引进了独立随机变量的和：$
$$X=X_1+\cdots+X_n.$$

$\qquad 我们证明了$
$$E[X]=E[X_1]+\cdots+E[X_n], \qquad var(X)=var(X_1)+\cdots+var(X_n)$$

$$\qquad上述公式中，关于随机变量和的期望的公式，并不要求随机变量之间的独立性，$$

$但是关于随机变量的和的方差的公式却要求随机变量之间的独立性。$
