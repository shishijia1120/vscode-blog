# 第三章 线性模型
## 3.1 基本形式
$给定由d个属性描述的示例x=(x_1;x_2;...;x_d),其中x_i是x在第i个属性上的取值，线性模型(linear model)试图学得一个通过属性的线性组合来进行预测的函数，即$

$$f(x)=\omega_1x_1+\omega_2 x_2+...+\omega_d x_d+b, (3.1)$$

$一般用向量形式写成$
$$f(x)=\omega^Tx+b,(3.2)$$
$其中\omega=(\omega_1;\omega_2;...;\omega_d).\omega和b学得之后，模型就得以确定.$
## 3.2 线性回归
### 3.2.1 单变量线性回归
$先考虑一种最简单的情形：输入属性的数目只有一个。$
$线性回归学得$
$$f(x_i)=\omega x_i+b，使得f(x_i)\simeq y_i.(3.3)$$
$如何确定\omega和b呢？显然，关键在于如何衡量f(x)与y之间的差别.均方差(2.2)是回归任务中最常用的性能度量，因此我们可试图让均方误差最小化，即$
$$(\omega^*,b^*)=\arg_{(\omega,b)}\min\sum_{i=1}^m(f(x_i)-y_i)^2=\arg_{(\omega,b)}\min\sum_{i=1}^m(y_i-\omega x_i-b)^2 (3.4)$$
$均方误差有非常的几何意义，它对应了常用的欧几里得距离或简称“欧氏距离”.基于均方误差最小化来进行模型求解的方法称为'最小二乘法'.在线性回归中，最小二乘法就是试图找到一条直线，使所有样本到直线上的欧氏距离之和最小.$
$求解\omega和b使E_{(\omega,b)}=\sum_{i=1}^{m}(y_i-\omega x_i-b)^2最小化的过程，称为线性回归模型的最小二乘“参数估计”。我们可将E_{(\omega,b)}分别对\omega和b求导，得到$
$$\frac{\partial E_{(\omega,b)}}{\partial \omega}=2(\omega \sum_{i=1}^m x_i^2-\sum_{i=1}^{m}(y_i-b)x_i),(3.5)$$
$$\frac{\partial E_{(\omega,b)}}{\partial b}=2(mb-\sum_{i=1}^{m}(y_i-\omega x_i)),(3.6)
$$
$令(3.5)和(3.6)为零可得到\omega和b最优解的闭式解$
$$\omega = \frac{\sum_{i=1}^{m}y_i(x_i-\bar{x})}{\sum_{i=1}^{m}x_i^2-\frac{1}{m}(\sum_{i=1}^{m}x_i)^2},(3.7)$$
$$b=\frac{1}{m}\sum_{i=1}{m}(y_i-\omega x_i),(3.8)$$
$其中\bar{x}=\frac{1}{m}x_i为x的均值$
### 3.2.2 多变量(元)线性回归
$样本由d个属性描述，此时我们试图学得$
$$f(x_i)=\omega^Tx_i+b,使得f(x_i)\simeq y_i,$$
$把\omega 和b吸收入向量形式\hat \omega =(\omega;b)，相应的，把数据集D表示为一个m\times(d+1)大小的矩阵X，其中每行对于一个示例，该行前d个元素对应于示例的d 个属性值，最后一个元素恒置为1，即$
<!-- $$X=\begin{pmatrix} x_{11} & x_{12} & \cdots &x_{1d} & 1\\
x_{21} & x_{22} & \cdots & x_{2d}& 1 \\ \vdots & \vdots & \ddots & \vdots  & \vdots  \\x_{m1} & x_{m2} & \cdots & x_{md}& 1 
\end{pmatrix}=\begin{pmatrix} x_1^T & 1 \\ x_2^T & 1 \\ \vdots & \vdots \\x_m^T & 1\end{pmatrix}$$ -->

$把标记也写成向量形式y=(y_1;y_2;...y_m),有$
$$\hat\omega*=\arg_{\hat\omega}\min(y-X\hat\omega)^T(y-X\hat\omega).(3.9)$$
$令E\hat\omega=(y-X\hat\omega)^T(y-X\hat\omega),对\hat\omega求导得到$
$$\frac{\partial E\hat\omega}{\partial \hat \omega}=2X^T(X\hat\omega-y).(3.10)$$